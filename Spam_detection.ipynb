{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5B5FUgLX9F4EIGC/8BO83",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumeetsRoorkee/ML_Code/blob/main/Spam_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uj0VsbvqAlA"
      },
      "outputs": [],
      "source": [
        "! pip install tqdm\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from textblob import TextBlob\n",
        "import time\n",
        "import re\n",
        "import gensim\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "tqdm.pandas()\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/kaggle/input/email-classification-nlp/SMS_train.csv\", encoding='iso-8859-1')\n",
        "test_df = pd.read_csv(\"/kaggle/input/email-classification-nlp/SMS_test.csv\", encoding='iso-8859-1')"
      ],
      "metadata": {
        "id": "iNpiEM1KqDbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ],
      "metadata": {
        "id": "9CVhmJsNqHF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Label'].value_counts()"
      ],
      "metadata": {
        "id": "mvWlEbQCqONe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "train_df['Spam_Label']= label_encoder.fit_transform(train_df['Label'])\n",
        "test_df['Spam_Label']= label_encoder.fit_transform(test_df['Label'])\n",
        "train_df.head"
      ],
      "metadata": {
        "id": "X_mGS8xxqP_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head"
      ],
      "metadata": {
        "id": "rlBszEXgqRxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "omkEENIxqUB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "w_w7tzEWqV3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def proper_text(text):\n",
        "    text = text.lower() #converts to lower case\n",
        "    text = re.sub(r'\\s+', ' ', text) # Removes extra space\n",
        "    text = re.sub(r'[^\\w\\s]','',text) # Removes puncutations\n",
        "    text = re.sub(r'\\d+','',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "oF_NxJ0pqXeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['Message_body'] = df['Message_body'].progress_apply(proper_text)"
      ],
      "metadata": {
        "id": "qzGzywq6qZMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_correct(text):\n",
        "    textblob_obj = TextBlob(text)\n",
        "    return textblob_obj.correct().string"
      ],
      "metadata": {
        "id": "9gOV3xdyqa7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Message_body'] = df['Message_body'].progress_apply(spell_correct)"
      ],
      "metadata": {
        "id": "6LDZ6dxXqch1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "pBGqeF4YqeF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in stop_words:\n",
        "            result.append(token)\n",
        "    return result"
      ],
      "metadata": {
        "id": "V_KFFsmuqgIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['token'] = df['Message_body'].progress_apply(preprocess)"
      ],
      "metadata": {
        "id": "deJE0Ad_qhmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['token'][1]"
      ],
      "metadata": {
        "id": "jjvcd68HqjJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "for i in df['token']:\n",
        "    for j in i:\n",
        "        vocab.append(j)"
      ],
      "metadata": {
        "id": "GujWUlt4qlwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[0:10]"
      ],
      "metadata": {
        "id": "enPbvRQNqnS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(list(set(vocab)))\n",
        "total_words"
      ],
      "metadata": {
        "id": "miNIQ-qHqnzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_data'] = df['token'].apply(lambda x: \" \".join(x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lvda6bSeqpYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = -1\n",
        "for doc in df.clean_data:\n",
        "    token = nltk.word_tokenize(doc)\n",
        "    if(max_len<len(token)):\n",
        "        max_len = len(token)\n",
        "print(max_len)"
      ],
      "metadata": {
        "id": "kisXiPp9qqv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tokenizer = Tokenizer(num_words = total_words)\n",
        "tokenizer.fit_on_texts(df['clean_data'])\n",
        "sequence = tokenizer.texts_to_sequences(df['clean_data'])\n",
        "\n",
        "X = pad_sequences(sequence, maxlen=max_len)\n",
        "y = df['Spam_Label'].values\n"
      ],
      "metadata": {
        "id": "QRaobBoaqsOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "D5-_zyTMqt02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "f499SQQLqvg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(total_words, output_dim=128))\n",
        "\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RkCdIM6Xqw3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "HFmm64jyqzNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "EGx3zMwKq0-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if the predicted value is >0.5 it is real else it is fake\n",
        "prediction = []\n",
        "for i in range(len(pred)):\n",
        "    if pred[i].item() > 0.5:\n",
        "        prediction.append(1)\n",
        "    else:\n",
        "        prediction.append(0)"
      ],
      "metadata": {
        "id": "W4dT0eO4q2wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(list(y_test), prediction)\n",
        "\n",
        "print(\"Model Accuracy : \", accuracy)"
      ],
      "metadata": {
        "id": "RzG5VCGZq48S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(list(y_test), prediction)\n",
        "plt.figure(figsize = (25, 25))\n",
        "sns.heatmap(cm, annot = True)"
      ],
      "metadata": {
        "id": "CqY-ZyYvq6db"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
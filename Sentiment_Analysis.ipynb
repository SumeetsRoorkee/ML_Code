{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T19:52:00.458085Z","iopub.execute_input":"2025-05-29T19:52:00.458442Z","iopub.status.idle":"2025-05-29T19:52:02.926365Z","shell.execute_reply.started":"2025-05-29T19:52:00.458409Z","shell.execute_reply":"2025-05-29T19:52:02.925342Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-analysis-dataset/sentiment_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#loading libraries\n! pip install tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom textblob import TextBlob\nimport time\nimport re\nimport gensim\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\ntqdm.pandas()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:29:52.294236Z","iopub.execute_input":"2025-05-29T20:29:52.294549Z","iopub.status.idle":"2025-05-29T20:29:56.239545Z","shell.execute_reply.started":"2025-05-29T20:29:52.294525Z","shell.execute_reply":"2025-05-29T20:29:56.237832Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n/kaggle/input/sentiment-analysis-dataset/sentiment_data.csv\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/sentiment_data.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:48:45.749239Z","iopub.execute_input":"2025-05-29T20:48:45.749610Z","iopub.status.idle":"2025-05-29T20:48:46.524203Z","shell.execute_reply.started":"2025-05-29T20:48:45.749582Z","shell.execute_reply":"2025-05-29T20:48:46.522826Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                            Comment  Sentiment\n0           0  lets forget apple pay required brand new iphon...          1\n1           1  nz retailers don’t even contactless credit car...          0\n2           2  forever acknowledge channel help lessons ideas...          2\n3           3  whenever go place doesn’t take apple pay doesn...          0\n4           4  apple pay convenient secure easy use used kore...          2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Comment</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>lets forget apple pay required brand new iphon...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>nz retailers don’t even contactless credit car...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>forever acknowledge channel help lessons ideas...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>whenever go place doesn’t take apple pay doesn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>apple pay convenient secure easy use used kore...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"df['Comment'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:45:33.928368Z","iopub.execute_input":"2025-05-29T20:45:33.928713Z","iopub.status.idle":"2025-05-29T20:45:33.936211Z","shell.execute_reply.started":"2025-05-29T20:45:33.928689Z","shell.execute_reply":"2025-05-29T20:45:33.935194Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"'lets forget apple pay required brand new iphone order use significant portion apples user base wasnt able use even wanted successive iphone incorporated technology older iphones replaced number people could use technology increased'"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:12:31.116230Z","iopub.execute_input":"2025-05-29T20:12:31.116625Z","iopub.status.idle":"2025-05-29T20:12:31.126539Z","shell.execute_reply.started":"2025-05-29T20:12:31.116594Z","shell.execute_reply":"2025-05-29T20:12:31.125172Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0     int64\nComment       object\nSentiment      int64\ndtype: object"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"df['Comment'].dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:48:55.261222Z","iopub.execute_input":"2025-05-29T20:48:55.261581Z","iopub.status.idle":"2025-05-29T20:48:55.300579Z","shell.execute_reply.started":"2025-05-29T20:48:55.261555Z","shell.execute_reply":"2025-05-29T20:48:55.299396Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"0         lets forget apple pay required brand new iphon...\n1         nz retailers don’t even contactless credit car...\n2         forever acknowledge channel help lessons ideas...\n3         whenever go place doesn’t take apple pay doesn...\n4         apple pay convenient secure easy use used kore...\n                                ...                        \n241140    crores paid neerav modi recovered congress lea...\n241141    dear rss terrorist payal gawar modi killing pl...\n241142                         cover interaction forum left\n241143    big project came india modi dream project happ...\n241144    ever listen like gurukul discipline maintained...\nName: Comment, Length: 240928, dtype: object"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:45:40.761827Z","iopub.execute_input":"2025-05-29T20:45:40.762230Z","iopub.status.idle":"2025-05-29T20:45:40.768631Z","shell.execute_reply.started":"2025-05-29T20:45:40.762194Z","shell.execute_reply":"2025-05-29T20:45:40.767542Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"(241145, 3)"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"#Cleaning Data\ndef proper_text(text):\n    if isinstance(text, str):  # Make sure the message is text\n        text = text.lower() #Converts to lower case\n        text = re.sub(r'[^\\w\\s]', '',text) #Removes puncutation\n        text = re.sub(r'\\d+','',text) # Removes digits\n    else:\n        return ' ' \n    return text\ndf['Clean_data'] = df['Comment'].progress_apply(proper_text)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:48:58.844370Z","iopub.execute_input":"2025-05-29T20:48:58.844719Z","iopub.status.idle":"2025-05-29T20:49:00.343328Z","shell.execute_reply.started":"2025-05-29T20:48:58.844696Z","shell.execute_reply":"2025-05-29T20:49:00.342087Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 241145/241145 [00:01<00:00, 162956.94it/s]\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:49:02.108369Z","iopub.execute_input":"2025-05-29T20:49:02.108684Z","iopub.status.idle":"2025-05-29T20:49:02.119387Z","shell.execute_reply.started":"2025-05-29T20:49:02.108660Z","shell.execute_reply":"2025-05-29T20:49:02.118201Z"}},"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                            Comment  Sentiment  \\\n0           0  lets forget apple pay required brand new iphon...          1   \n1           1  nz retailers don’t even contactless credit car...          0   \n2           2  forever acknowledge channel help lessons ideas...          2   \n3           3  whenever go place doesn’t take apple pay doesn...          0   \n4           4  apple pay convenient secure easy use used kore...          2   \n\n                                          Clean_data  \n0  lets forget apple pay required brand new iphon...  \n1  nz retailers dont even contactless credit card...  \n2  forever acknowledge channel help lessons ideas...  \n3  whenever go place doesnt take apple pay doesnt...  \n4  apple pay convenient secure easy use used kore...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Comment</th>\n      <th>Sentiment</th>\n      <th>Clean_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>lets forget apple pay required brand new iphon...</td>\n      <td>1</td>\n      <td>lets forget apple pay required brand new iphon...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>nz retailers don’t even contactless credit car...</td>\n      <td>0</td>\n      <td>nz retailers dont even contactless credit card...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>forever acknowledge channel help lessons ideas...</td>\n      <td>2</td>\n      <td>forever acknowledge channel help lessons ideas...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>whenever go place doesn’t take apple pay doesn...</td>\n      <td>0</td>\n      <td>whenever go place doesnt take apple pay doesnt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>apple pay convenient secure easy use used kore...</td>\n      <td>2</td>\n      <td>apple pay convenient secure easy use used kore...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\nlemmatizer = WordNetLemmatizer()\ndef tokenize(text):\n    if isinstance(text, str): \n        return tokenizer.tokenize(text)\n    else:\n        return ' '","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:49:07.209623Z","iopub.execute_input":"2025-05-29T20:49:07.209909Z","iopub.status.idle":"2025-05-29T20:49:07.215435Z","shell.execute_reply.started":"2025-05-29T20:49:07.209890Z","shell.execute_reply":"2025-05-29T20:49:07.214216Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"df['Clean_data'] = df['Clean_data'].progress_apply(tokenize)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:49:10.770800Z","iopub.execute_input":"2025-05-29T20:49:10.771234Z","iopub.status.idle":"2025-05-29T20:49:12.525760Z","shell.execute_reply.started":"2025-05-29T20:49:10.771208Z","shell.execute_reply":"2025-05-29T20:49:12.524547Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 241145/241145 [00:01<00:00, 139270.63it/s]\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:49:16.817059Z","iopub.execute_input":"2025-05-29T20:49:16.817454Z","iopub.status.idle":"2025-05-29T20:49:16.832388Z","shell.execute_reply.started":"2025-05-29T20:49:16.817428Z","shell.execute_reply":"2025-05-29T20:49:16.830866Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                            Comment  Sentiment  \\\n0           0  lets forget apple pay required brand new iphon...          1   \n1           1  nz retailers don’t even contactless credit car...          0   \n2           2  forever acknowledge channel help lessons ideas...          2   \n3           3  whenever go place doesn’t take apple pay doesn...          0   \n4           4  apple pay convenient secure easy use used kore...          2   \n\n                                          Clean_data  \n0  [lets, forget, apple, pay, required, brand, ne...  \n1  [nz, retailers, dont, even, contactless, credi...  \n2  [forever, acknowledge, channel, help, lessons,...  \n3  [whenever, go, place, doesnt, take, apple, pay...  \n4  [apple, pay, convenient, secure, easy, use, us...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Comment</th>\n      <th>Sentiment</th>\n      <th>Clean_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>lets forget apple pay required brand new iphon...</td>\n      <td>1</td>\n      <td>[lets, forget, apple, pay, required, brand, ne...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>nz retailers don’t even contactless credit car...</td>\n      <td>0</td>\n      <td>[nz, retailers, dont, even, contactless, credi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>forever acknowledge channel help lessons ideas...</td>\n      <td>2</td>\n      <td>[forever, acknowledge, channel, help, lessons,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>whenever go place doesn’t take apple pay doesn...</td>\n      <td>0</td>\n      <td>[whenever, go, place, doesnt, take, apple, pay...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>apple pay convenient secure easy use used kore...</td>\n      <td>2</td>\n      <td>[apple, pay, convenient, secure, easy, use, us...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"vocab = []\nfor i in df.Clean_data:\n    for j in i:\n        vocab.append(j)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:57:57.973846Z","iopub.execute_input":"2025-05-29T20:57:57.974749Z","iopub.status.idle":"2025-05-29T20:57:58.275705Z","shell.execute_reply.started":"2025-05-29T20:57:57.974710Z","shell.execute_reply":"2025-05-29T20:57:58.274490Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"vocab[0:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:58:10.214880Z","iopub.execute_input":"2025-05-29T20:58:10.215350Z","iopub.status.idle":"2025-05-29T20:58:10.222852Z","shell.execute_reply.started":"2025-05-29T20:58:10.215317Z","shell.execute_reply":"2025-05-29T20:58:10.221826Z"}},"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"['lets',\n 'forget',\n 'apple',\n 'pay',\n 'required',\n 'brand',\n 'new',\n 'iphone',\n 'order',\n 'use']"},"metadata":{}}],"execution_count":90},{"cell_type":"code","source":"total_words = len(list(set(vocab)))\ntotal_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T20:59:19.110230Z","iopub.execute_input":"2025-05-29T20:59:19.110608Z","iopub.status.idle":"2025-05-29T20:59:19.434058Z","shell.execute_reply.started":"2025-05-29T20:59:19.110579Z","shell.execute_reply":"2025-05-29T20:59:19.432846Z"}},"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"155381"},"metadata":{}}],"execution_count":92},{"cell_type":"code","source":"max_len = -1\nfor doc in df.Comment:\n    token = nltk.word_tokenize(doc)\n    if(max_len<len(token)):\n        max_len = len(token)\n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T21:09:46.264066Z","iopub.execute_input":"2025-05-29T21:09:46.264460Z","iopub.status.idle":"2025-05-29T21:09:46.409461Z","shell.execute_reply.started":"2025-05-29T21:09:46.264433Z","shell.execute_reply":"2025-05-29T21:09:46.407836Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3748774347.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[1;32m    119\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \"\"\"\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \"\"\"\n\u001b[1;32m   1456\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mprevious_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mprevious_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m             \u001b[0;31m# Get the slice of the previous word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m             \u001b[0mbefore_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprevious_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"],"ename":"TypeError","evalue":"expected string or bytes-like object, got 'float'","output_type":"error"}],"execution_count":108},{"cell_type":"code","source":"print(max_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T21:08:56.568358Z","iopub.execute_input":"2025-05-29T21:08:56.568681Z","iopub.status.idle":"2025-05-29T21:08:56.574468Z","shell.execute_reply.started":"2025-05-29T21:08:56.568657Z","shell.execute_reply":"2025-05-29T21:08:56.573408Z"}},"outputs":[{"name":"stdout","text":"-1\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntokenizer = Tokenizer(num_words = total_words)\ntokenizer.fit_on_texts(df['Clean_data'])\nsequence = tokenizer.texts_to_sequences(df['Clean_data'])\n\nX = pad_sequences(sequence, maxlen=max_len)\ny = df['Sentiment'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T21:10:56.083031Z","iopub.execute_input":"2025-05-29T21:10:56.083423Z","iopub.status.idle":"2025-05-29T21:11:02.280256Z","shell.execute_reply.started":"2025-05-29T21:10:56.083397Z","shell.execute_reply":"2025-05-29T21:11:02.279027Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T21:11:56.955174Z","iopub.execute_input":"2025-05-29T21:11:56.955481Z","iopub.status.idle":"2025-05-29T21:11:57.104875Z","shell.execute_reply.started":"2025-05-29T21:11:56.955460Z","shell.execute_reply":"2025-05-29T21:11:57.103688Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}